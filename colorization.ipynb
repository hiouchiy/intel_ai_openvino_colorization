{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 白黒画像の自動色付けサンプル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前学習済みモデルをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --list $INTEL_OPENVINO_DIR/inference_engine/demos/python_demos/colorization_demo/models.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前学習済みモデルをOpenVINOのIRに変換\n",
    "事前学習モデルは今回のように元のディープラーニングフレームワークのファイル形式で提供されている場合もあります。その場合は下記コマンドを使用して、OpenVINOの独自形式であるIR形式に変換ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/converter.py --name colorization-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "import sys\n",
    "import io\n",
    "\n",
    "from openvino.inference_engine import IECore\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorization:\n",
    "    def __init__(self, model_path):\n",
    "        # Plugin initialization\n",
    "        ie = IECore()\n",
    "    \n",
    "        # Setup OpenVINO's IE\n",
    "        model = model_path\n",
    "        net = ie.read_network(model, os.path.splitext(model)[0] + \".bin\")\n",
    "        self.input_blob = next(iter(net.input_info))\n",
    "        self.output_blob = next(iter(net.outputs))\n",
    "        self.input_batch_size, self.input_channel, self.input_height, self.input_width = net.input_info[self.input_blob].input_data.shape\n",
    "        self.exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "        \n",
    "        # Setup coeffs\n",
    "        coeffs = \"public/colorization-v2/colorization-v2.npy\"\n",
    "        self.color_coeff = np.load(coeffs).astype(np.float32)\n",
    "        assert self.color_coeff.shape == (313, 2), \"Current shape of color coefficients does not match required shape\"\n",
    "        \n",
    "    def __preprocess_input__(self, original_frame):\n",
    "        if original_frame.shape[2] > 1:\n",
    "            frame = cv2.cvtColor(cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            frame = cv2.cvtColor(original_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        img_rgb = frame.astype(np.float32) / 255\n",
    "        img_lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Lab)\n",
    "        img_l_rs = cv2.resize(img_lab.copy(), (self.input_width, self.input_height))[:, :, 0]\n",
    "        \n",
    "        return img_lab, img_l_rs \n",
    "    \n",
    "    def infer(self, original_frame):\n",
    "        # Read and pre-process input image (NOTE: one image only)\n",
    "        img_lab, img_l_rs = self.__preprocess_input__(original_frame)\n",
    "        \n",
    "        res = self.exec_net.infer(inputs={self.input_blob: [img_l_rs]})\n",
    "        \n",
    "        update_res = (res[self.output_blob] * self.color_coeff.transpose()[:, :, np.newaxis, np.newaxis]).sum(1)\n",
    "\n",
    "        out = update_res.transpose((1, 2, 0))\n",
    "        (h_orig, w_orig) = original_frame.shape[:2]\n",
    "        out = cv2.resize(out, (w_orig, h_orig))\n",
    "        img_lab_out = np.concatenate((img_lab[:, :, 0][:, :, np.newaxis], out), axis=2)\n",
    "        img_bgr_out = np.clip(cv2.cvtColor(img_lab_out, cv2.COLOR_Lab2BGR), 0, 1)\n",
    "        \n",
    "        return img_bgr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_image(original_frame, img_bgr_out):\n",
    "    (h_orig, w_orig) = original_frame.shape[:2]\n",
    "    imshowSize = (int(w_orig*(200/h_orig)), 200)\n",
    "    original_image = cv2.resize(original_frame, imshowSize)\n",
    "    colorize_image = (cv2.resize(img_bgr_out, imshowSize) * 255).astype(np.uint8)\n",
    "\n",
    "    original_image = cv2.putText(original_image, 'Original', (25, 50),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    colorize_image = cv2.putText(colorize_image, 'Colorize', (25, 50),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    ir_image = [cv2.hconcat([original_image, colorize_image])]\n",
    "    final_image = cv2.vconcat(ir_image)\n",
    "    final_image = cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン処理を実装（単一画像の推論）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorization = Colorization(\"public/colorization-v2/FP32/colorization-v2.xml\")\n",
    "\n",
    "input_source = \"sample.jpg\"\n",
    "original_frame = cv2.imread(input_source)\n",
    "\n",
    "img_bgr_out = colorization.infer(original_frame)\n",
    "\n",
    "final_image = create_output_image(original_frame, img_bgr_out)\n",
    "\n",
    "f = io.BytesIO()\n",
    "PIL.Image.fromarray(final_image).save(f, 'jpeg')\n",
    "IPython.display.display(IPython.display.Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン処理を実装（動画の推論）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorization = Colorization(\"public/colorization-v2/FP32/colorization-v2.xml\")\n",
    "\n",
    "input_source = \"https://github.com/hiouchiy/IntelAI_and_Cloud/raw/master/Azure/handson/bw.mp4\"\n",
    "cap = cv2.VideoCapture(input_source)\n",
    "if not cap.isOpened():\n",
    "    assert \"{} not exist\".format(input_source)\n",
    "\n",
    "while True:\n",
    "    hasFrame, original_frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        break\n",
    "\n",
    "    img_bgr_out = colorization.infer(original_frame)\n",
    "\n",
    "    final_image = create_output_image(original_frame, img_bgr_out)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(final_image).save(f, 'jpeg')\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "    if not cv2.waitKey(1) < 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ Model Serverを使用してモデルをマイクロサービス化\n",
    "ここからは手書き文字認識モデルをOpenVINO Model Serverでマイクロサービス化して外出しにします。Model ServerとはgRPCを介してコミュニケーションを取ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO Model Serverのセットアップ\n",
    "OpenVINO Model ServerをホストOS上で稼働させる手順です。こちらを実行後に以降の作業を進めてください。\n",
    "1. Model ServerのDockerイメージをダウンロード\n",
    "```Bash\n",
    "docker pull openvino/model_server:latest\n",
    "```\n",
    "1. 自動色付け用モデル（XMLファイルとBINファイル）を適当なフォルダへ格納\n",
    "1. Model Serverを起動\n",
    "```Bash\n",
    "docker run -d --rm -v C:\\Users\\hiroshi\\model\\colorization:/models/colorization/1 -p 9000:9000 openvino/model_server:latest --model_path /models/colorization --model_name colorization --port 9000 --log_level DEBUG  --shape auto\n",
    "```\n",
    "※\"C:\\Users\\hiroshi\\model\\colorization\"にモデルが格納されているとした場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import grpc\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "from tensorflow import make_tensor_proto, make_ndarray\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc, get_model_metadata_pb2\n",
    "\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class RemoteColorization:\n",
    "    def __init__(self, grpc_address='localhost', grpc_port=9000, model_name='colorization', model_version=None):\n",
    "        \n",
    "        #Settings for accessing model server\n",
    "        self.grpc_address = grpc_address\n",
    "        self.grpc_port = grpc_port\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        channel = grpc.insecure_channel(\"{}:{}\".format(self.grpc_address, self.grpc_port))\n",
    "        self.stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "        \n",
    "        # Get input shape info from Model Server\n",
    "        self.input_name, input_shape, self.output_name, output_shape = self.__get_input_name_and_shape__()\n",
    "        self.input_batchsize = input_shape[0]\n",
    "        self.input_channel = input_shape[1]\n",
    "        self.input_height = input_shape[2]\n",
    "        self.input_width = input_shape[3]\n",
    "        \n",
    "        # Setup coeffs\n",
    "        coeffs = \"public/colorization-v2/colorization-v2.npy\"\n",
    "        self.color_coeff = np.load(coeffs).astype(np.float32)\n",
    "        assert self.color_coeff.shape == (313, 2), \"Current shape of color coefficients does not match required shape\"\n",
    "\n",
    "    def __get_input_name_and_shape__(self):\n",
    "        metadata_field = \"signature_def\"\n",
    "        request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "        request.model_spec.name = self.model_name\n",
    "        if self.model_version is not None:\n",
    "            request.model_spec.version.value = self.model_version\n",
    "        request.metadata_field.append(metadata_field)\n",
    "\n",
    "        result = self.stub.GetModelMetadata(request, 10.0) # result includes a dictionary with all model outputs\n",
    "        input_metadata, output_metadata = self.__get_input_and_output_meta_data__(result)\n",
    "        input_blob = next(iter(input_metadata.keys()))\n",
    "        output_blob = next(iter(output_metadata.keys()))\n",
    "        return input_blob, input_metadata[input_blob]['shape'], output_blob, output_metadata[output_blob]['shape']\n",
    "    \n",
    "    def __get_input_and_output_meta_data__(self, response):\n",
    "        signature_def = response.metadata['signature_def']\n",
    "        signature_map = get_model_metadata_pb2.SignatureDefMap()\n",
    "        signature_map.ParseFromString(signature_def.value)\n",
    "        serving_default = signature_map.ListFields()[0][1]['serving_default']\n",
    "        serving_inputs = serving_default.inputs\n",
    "        input_blobs_keys = {key: {} for key in serving_inputs.keys()}\n",
    "        tensor_shape = {key: serving_inputs[key].tensor_shape\n",
    "                        for key in serving_inputs.keys()}\n",
    "        for input_blob in input_blobs_keys:\n",
    "            inputs_shape = [d.size for d in tensor_shape[input_blob].dim]\n",
    "            tensor_dtype = serving_inputs[input_blob].dtype\n",
    "            input_blobs_keys[input_blob].update({'shape': inputs_shape})\n",
    "            input_blobs_keys[input_blob].update({'dtype': tensor_dtype})\n",
    "        \n",
    "        serving_outputs = serving_default.outputs\n",
    "        output_blobs_keys = {key: {} for key in serving_outputs.keys()}\n",
    "        tensor_shape = {key: serving_outputs[key].tensor_shape\n",
    "                        for key in serving_outputs.keys()}\n",
    "        for output_blob in output_blobs_keys:\n",
    "            outputs_shape = [d.size for d in tensor_shape[output_blob].dim]\n",
    "            tensor_dtype = serving_outputs[output_blob].dtype\n",
    "            output_blobs_keys[output_blob].update({'shape': outputs_shape})\n",
    "            output_blobs_keys[output_blob].update({'dtype': tensor_dtype})\n",
    "\n",
    "        return input_blobs_keys, output_blobs_keys\n",
    "        \n",
    "    def __preprocess_input__(self, original_frame):\n",
    "        if original_frame.shape[2] > 1:\n",
    "            frame = cv2.cvtColor(cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            frame = cv2.cvtColor(original_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        img_rgb = frame.astype(np.float32) / 255\n",
    "        img_lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Lab)\n",
    "        img_l_rs = cv2.resize(img_lab.copy(), (self.input_width, self.input_height))[:, :, 0]\n",
    "        \n",
    "        return img_lab, img_l_rs \n",
    "    \n",
    "    def infer(self, original_frame):\n",
    "        # Read and pre-process input image (NOTE: one image only)\n",
    "        img_lab, img_l_rs = self.__preprocess_input__(original_frame)\n",
    "        input_image = img_l_rs.reshape(self.input_batchsize, self.input_channel, self.input_height, self.input_width).astype(np.float32)\n",
    "        \n",
    "        #res = self.exec_net.infer(inputs={self.input_blob: [img_l_rs]})\n",
    "        # Model ServerにgRPCでアクセスしてモデルをコール\n",
    "        request = predict_pb2.PredictRequest()\n",
    "        request.model_spec.name = self.model_name\n",
    "        request.inputs[self.input_name].CopyFrom(make_tensor_proto(input_image, shape=(input_image.shape)))\n",
    "        result = self.stub.Predict(request, 10.0) # result includes a dictionary with all model outputs\n",
    "        res = make_ndarray(result.outputs[self.output_name])\n",
    "        \n",
    "        update_res = (res * self.color_coeff.transpose()[:, :, np.newaxis, np.newaxis]).sum(1)\n",
    "\n",
    "        out = update_res.transpose((1, 2, 0))\n",
    "        (h_orig, w_orig) = original_frame.shape[:2]\n",
    "        out = cv2.resize(out, (w_orig, h_orig))\n",
    "        img_lab_out = np.concatenate((img_lab[:, :, 0][:, :, np.newaxis], out), axis=2)\n",
    "        img_bgr_out = np.clip(cv2.cvtColor(img_lab_out, cv2.COLOR_Lab2BGR), 0, 1)\n",
    "        \n",
    "        return img_bgr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorization = RemoteColorization(grpc_address='192.168.145.33', grpc_port='9000', model_name='colorization')\n",
    "\n",
    "input_source = \"sample.jpg\"\n",
    "original_frame = cv2.imread(input_source)\n",
    "\n",
    "img_bgr_out = colorization.infer(original_frame)\n",
    "\n",
    "final_image = create_output_image(original_frame, img_bgr_out)\n",
    "\n",
    "f = io.BytesIO()\n",
    "PIL.Image.fromarray(final_image).save(f, 'jpeg')\n",
    "IPython.display.display(IPython.display.Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
